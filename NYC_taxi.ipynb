{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5e57f3-c75d-49ac-8b6f-d9dcbda3b384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # üöï Projet Taxi NYC - Analyse Comparative\n",
    "# MAGIC ## Statistiques Inf√©rentielles vs Big Data\n",
    "# MAGIC \n",
    "# MAGIC **Entreprise:** DATACO  \n",
    "# MAGIC **P√©riode:** 2022-2025  \n",
    "# MAGIC **Bin√¥me:** [Vos noms]  \n",
    "# MAGIC **Date:** 26-30 janvier 2026\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üì¶ 1. Configuration & Imports\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Imports PySpark\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, mean, stddev, sum as spark_sum, min as spark_min, max as spark_max,\n",
    "    hour, dayofweek, dayofmonth, month, year, weekofyear,\n",
    "    unix_timestamp, percentile_approx, when, lit\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Imports Python scientifique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "# Imports visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a7dd61-7249-4c0b-b9ce-ad19b336bdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üìÇ 2. Chargement des Donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, LongType\n",
    "\n",
    "# CHEMINS DES DONN√âES\n",
    "PATH_POPULATION = \"/Volumes/workspace/trips/population/\"\n",
    "PATH_SAMPLE = \"/Volumes/workspace/trips/sample/\"\n",
    "\n",
    "# ============================================\n",
    "# CHARGEMENT √âCHANTILLON (CSV)\n",
    "# ============================================\n",
    "print(\"üîÑ Chargement de l'√©chantillon...\")\n",
    "df_sample = spark.read.csv(\n",
    "    PATH_SAMPLE + \"yellowtaxisample1pct_hybrid_stratified.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "nb_sample = df_sample.count()\n",
    "print(f\"‚úÖ √âchantillon charg√© : {nb_sample:,} courses\")\n",
    "\n",
    "# ============================================\n",
    "# CHARGEMENT POPULATION - Approche fichier par fichier\n",
    "# ============================================\n",
    "print(\"\\nüîÑ Chargement de la population compl√®te...\")\n",
    "\n",
    "# Lister tous les fichiers parquet\n",
    "files = [f.path for f in dbutils.fs.ls(PATH_POPULATION) if f.path.endswith('.parquet')]\n",
    "print(f\"üìÇ {len(files)} fichiers trouv√©s\")\n",
    "\n",
    "# Charger tous les fichiers avec conversion automatique\n",
    "dfs = []\n",
    "for i, file_path in enumerate(files, 1):\n",
    "    try:\n",
    "        # Lire le fichier\n",
    "        df_temp = spark.read.parquet(file_path)\n",
    "        \n",
    "        # Convertir les colonnes qui peuvent √™tre INT64 ou DOUBLE\n",
    "        columns_to_convert = [\n",
    "            \"passenger_count\", \"trip_distance\", \"RatecodeID\",\n",
    "            \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \n",
    "            \"tolls_amount\", \"improvement_surcharge\", \"total_amount\",\n",
    "            \"congestion_surcharge\", \"airport_fee\"\n",
    "        ]\n",
    "        \n",
    "        for col_name in columns_to_convert:\n",
    "            if col_name in df_temp.columns:\n",
    "                df_temp = df_temp.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "        \n",
    "        dfs.append(df_temp)\n",
    "        print(f\"‚úÖ [{i}/{len(files)}] {file_path.split('/')[-1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur sur {file_path.split('/')[-1]}: {str(e)}\")\n",
    "\n",
    "# Fusionner tous les DataFrames\n",
    "if dfs:\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import DataFrame\n",
    "    \n",
    "    print(\"\\nüîÑ Fusion de tous les fichiers...\")\n",
    "    df_population = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "    \n",
    "    nb_population = df_population.count()\n",
    "    print(f\"‚úÖ Population totale charg√©e : {nb_population:,} courses\")\n",
    "else:\n",
    "    raise Exception(\"‚ùå Aucun fichier n'a pu √™tre charg√©\")\n",
    "\n",
    "# V√©rification ratio\n",
    "ratio = (nb_sample / nb_population) * 100\n",
    "print(f\"\\nüìä Ratio √©chantillon/population : {ratio:.2f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# V√©rification des sch√©mas finaux\n",
    "print(\"=== SCH√âMA POPULATION ===\")\n",
    "df_population.printSchema()\n",
    "\n",
    "print(\"\\n=== SCH√âMA √âCHANTILLON ===\")\n",
    "df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23e93f9-7ca9-4d50-8a06-2c656d1b921a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üîç 3. EDA - Exploration des Donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.1 Structure des donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== SCH√âMA POPULATION ===\")\n",
    "df_population.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.2 Aper√ßu des donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== APER√áU POPULATION (5 premi√®res lignes) ===\")\n",
    "display(df_population.limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.3 Statistiques descriptives de base\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Population\n",
    "print(\"=== STATISTIQUES DESCRIPTIVES - POPULATION ===\")\n",
    "stats_pop = df_population.select(\n",
    "    \"fare_amount\", \"trip_distance\", \"tip_amount\", \n",
    "    \"tolls_amount\", \"total_amount\", \"passenger_count\"\n",
    ").describe()\n",
    "display(stats_pop)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.4 V√©rification des valeurs manquantes\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, sum as spark_sum, isnan, when, count\n",
    "# Fonction pour compter valeurs manquantes\n",
    "def count_nulls(df, dataset_name):\n",
    "    print(f\"\\n=== VALEURS MANQUANTES - {dataset_name} ===\")\n",
    "    null_counts = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "        for c in df.columns\n",
    "    ])\n",
    "    \n",
    "    # Conversion en pandas pour affichage plus lisible\n",
    "    null_df = null_counts.toPandas().T\n",
    "    null_df.columns = ['Nb_Nulls']\n",
    "    null_df['Pct_Nulls'] = (null_df['Nb_Nulls'] / df.count() * 100).round(2)\n",
    "    null_df = null_df[null_df['Nb_Nulls'] > 0].sort_values('Nb_Nulls', ascending=False)\n",
    "    \n",
    "    if len(null_df) > 0:\n",
    "        print(null_df)\n",
    "    else:\n",
    "        print(\"‚úÖ Aucune valeur manquante d√©tect√©e\")\n",
    "    \n",
    "    return null_df\n",
    "\n",
    "# V√©rification\n",
    "nulls_pop = count_nulls(df_population, \"POPULATION\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.5 D√©tection pr√©liminaire des outliers\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul des quartiles pour fare_amount\n",
    "quantiles_fare = df_population.approxQuantile(\"fare_amount\", [0.01, 0.25, 0.50, 0.75, 0.99], 0.01)\n",
    "Q1, Q3 = quantiles_fare[1], quantiles_fare[3]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"=== ANALYSE FARE_AMOUNT ===\")\n",
    "print(f\"Q1 (25%): ${Q1:.2f}\")\n",
    "print(f\"M√©diane (50%): ${quantiles_fare[2]:.2f}\")\n",
    "print(f\"Q3 (75%): ${Q3:.2f}\")\n",
    "print(f\"IQR: ${IQR:.2f}\")\n",
    "print(f\"1er percentile: ${quantiles_fare[0]:.2f}\")\n",
    "print(f\"99e percentile: ${quantiles_fare[4]:.2f}\")\n",
    "\n",
    "# Limites outliers (m√©thode IQR)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f\"\\nLimites outliers (IQR ¬±1.5):\")\n",
    "print(f\"Limite inf√©rieure: ${lower_bound:.2f}\")\n",
    "print(f\"Limite sup√©rieure: ${upper_bound:.2f}\")\n",
    "\n",
    "# Comptage outliers\n",
    "nb_outliers = df_population.filter(\n",
    "    (col(\"fare_amount\") < lower_bound) | (col(\"fare_amount\") > upper_bound)\n",
    ").count()\n",
    "pct_outliers = (nb_outliers / nb_population) * 100\n",
    "\n",
    "print(f\"\\nüìä Outliers d√©tect√©s: {nb_outliers:,} ({pct_outliers:.2f}%)\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7ed46a-6b64-43d8-b0a4-4d08579e5cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## üíæ 5. Analyse Big Data (Population Compl√®te)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.1 Calcul des m√©triques EXACTES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, mean, unix_timestamp, count as spark_count, sum as spark_sum\n",
    "\n",
    "# Mesure du temps de calcul\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=== CALCUL DES M√âTRIQUES EXACTES (POPULATION) ===\")\n",
    "print(\"üîÑ Calcul en cours...\")\n",
    "\n",
    "# Prix moyen exact\n",
    "mean_fare_exact = df_population.agg(mean(\"fare_amount\")).collect()[0][0]\n",
    "print(f\"‚úÖ Prix moyen EXACT: ${mean_fare_exact:.2f}\")\n",
    "\n",
    "# Distance moyenne exacte\n",
    "mean_distance_exact = df_population.agg(mean(\"trip_distance\")).collect()[0][0]\n",
    "print(f\"‚úÖ Distance moyenne EXACTE: {mean_distance_exact:.2f} miles\")\n",
    "\n",
    "# Dur√©e moyenne exacte\n",
    "# ‚úÖ CORRECTION : Utiliser unix_timestamp() au lieu de cast(\"long\")\n",
    "df_population_duration = df_population.withColumn(\n",
    "    \"duration_minutes\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
    ")\n",
    "\n",
    "# V√©rification rapide\n",
    "print(\"\\nüìä Aper√ßu des dur√©es calcul√©es:\")\n",
    "df_population_duration.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"duration_minutes\").show(5)\n",
    "\n",
    "# Calcul de la moyenne exacte\n",
    "mean_duration_exact = df_population_duration.agg(mean(\"duration_minutes\")).collect()[0][0]\n",
    "print(f\"‚úÖ Dur√©e moyenne EXACTE: {mean_duration_exact:.2f} minutes\")\n",
    "\n",
    "# Proportion exacte avec tip\n",
    "nb_with_tip_exact = df_population.filter(col(\"tip_amount\") > 0).count()\n",
    "prop_tip_exact = nb_with_tip_exact / nb_population\n",
    "print(f\"‚úÖ Proportion EXACTE avec tip: {prop_tip_exact:.2%}\")\n",
    "\n",
    "# Temps de calcul\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Temps de calcul: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "# Stockage pour comparaison\n",
    "results_bigdata = {\n",
    "    'mean_fare': mean_fare_exact,\n",
    "    'mean_distance': mean_distance_exact,\n",
    "    'mean_duration': mean_duration_exact,\n",
    "    'prop_tip': prop_tip_exact,\n",
    "    'compute_time': elapsed_time\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Toutes les m√©triques exactes calcul√©es!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.2 Distribution temporelle - Heures de pointe\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import hour, dayofweek, dayofmonth, month\n",
    "\n",
    "# Ajout des colonnes temporelles + dur√©e\n",
    "df_population_time = df_population.withColumn(\n",
    "    \"duration_minutes\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
    ").withColumn(\"hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    " .withColumn(\"dayofweek\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
    " .withColumn(\"day\", dayofmonth(\"tpep_pickup_datetime\")) \\\n",
    " .withColumn(\"month\", month(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Analyse par heure\n",
    "courses_par_heure = df_population_time.groupBy(\"hour\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "    mean(\"trip_distance\").alias(\"distance_moyenne\"),\n",
    "    mean(\"duration_minutes\").alias(\"duree_moyenne\")\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "print(\"=== DISTRIBUTION PAR HEURE ===\")\n",
    "display(courses_par_heure)\n",
    "\n",
    "# Identification heures de pointe\n",
    "print(\"\\n=== TOP 5 HEURES DE POINTE ===\")\n",
    "heures_pointe = courses_par_heure.orderBy(col(\"nb_courses\").desc()).limit(5)\n",
    "display(heures_pointe)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.3 Distribution par jour de la semaine\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Analyse par jour de semaine (1=Dimanche, 7=Samedi)\n",
    "courses_par_jour = df_population_time.groupBy(\"dayofweek\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "    mean(\"duration_minutes\").alias(\"duree_moyenne\")\n",
    ").orderBy(\"dayofweek\")\n",
    "\n",
    "# Mapping des jours (pour r√©f√©rence)\n",
    "jours_mapping = {\n",
    "    1: \"Dimanche\", 2: \"Lundi\", 3: \"Mardi\", 4: \"Mercredi\",\n",
    "    5: \"Jeudi\", 6: \"Vendredi\", 7: \"Samedi\"\n",
    "}\n",
    "\n",
    "print(\"=== DISTRIBUTION PAR JOUR DE LA SEMAINE ===\")\n",
    "print(\"(1=Dimanche, 2=Lundi, 3=Mardi, 4=Mercredi, 5=Jeudi, 6=Vendredi, 7=Samedi)\")\n",
    "display(courses_par_jour)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.4 Analyse g√©ographique par boroughs\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# V√©rifier si les colonnes borough existent\n",
    "if \"pickup_borough\" in df_population.columns and \"dropoff_borough\" in df_population.columns:\n",
    "    \n",
    "    # Analyse compl√®te par borough\n",
    "    analyse_boroughs = df_population.groupBy(\"pickup_borough\").agg(\n",
    "        count(\"*\").alias(\"nb_courses\"),\n",
    "        mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "        mean(\"trip_distance\").alias(\"distance_moyenne\"),\n",
    "        mean(\"tip_amount\").alias(\"tip_moyen\"),\n",
    "        (mean(\"tip_amount\") / mean(\"fare_amount\") * 100).alias(\"tip_pct_fare\")\n",
    "    ).orderBy(col(\"nb_courses\").desc())\n",
    "\n",
    "    print(\"=== ANALYSE PAR BOROUGH (PICKUP) ===\")\n",
    "    display(analyse_boroughs)\n",
    "\n",
    "    # Statistiques par paire origine-destination\n",
    "    top_routes = df_population.groupBy(\"pickup_borough\", \"dropoff_borough\").agg(\n",
    "        count(\"*\").alias(\"nb_courses\"),\n",
    "        mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "        mean(\"trip_distance\").alias(\"distance_moyenne\")\n",
    "    ).orderBy(col(\"nb_courses\").desc()).limit(10)\n",
    "\n",
    "    print(\"\\n=== TOP 10 ROUTES (ORIGINE ‚Üí DESTINATION) ===\")\n",
    "    display(top_routes)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Les colonnes 'pickup_borough' et 'dropoff_borough' ne sont pas pr√©sentes dans les donn√©es\")\n",
    "    print(\"üìç Analyse par zones (PULocationID / DOLocationID) √† la place:\")\n",
    "    \n",
    "    # Analyse par zones de pickup\n",
    "    analyse_zones = df_population.groupBy(\"PULocationID\").agg(\n",
    "        count(\"*\").alias(\"nb_courses\"),\n",
    "        mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "        mean(\"trip_distance\").alias(\"distance_moyenne\"),\n",
    "        mean(\"tip_amount\").alias(\"tip_moyen\")\n",
    "    ).orderBy(col(\"nb_courses\").desc()).limit(10)\n",
    "    \n",
    "    print(\"\\n=== TOP 10 ZONES DE PICKUP ===\")\n",
    "    display(analyse_zones)\n",
    "    \n",
    "    # Top routes par zones\n",
    "    top_routes_zones = df_population.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "        count(\"*\").alias(\"nb_courses\"),\n",
    "        mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "        mean(\"trip_distance\").alias(\"distance_moyenne\")\n",
    "    ).orderBy(col(\"nb_courses\").desc()).limit(10)\n",
    "    \n",
    "    print(\"\\n=== TOP 10 ROUTES (ZONES) ===\")\n",
    "    display(top_routes_zones)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.5 D√©tection et analyse des outliers\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul des percentiles pour plusieurs variables\n",
    "print(\"=== ANALYSE DES OUTLIERS ===\")\n",
    "\n",
    "# Fare amount\n",
    "quantiles_fare_full = df_population.approxQuantile(\"fare_amount\", [0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99], 0.01)\n",
    "Q1_fare, Q3_fare = quantiles_fare_full[2], quantiles_fare_full[4]\n",
    "IQR_fare = Q3_fare - Q1_fare\n",
    "lower_bound_fare = Q1_fare - 1.5 * IQR_fare\n",
    "upper_bound_fare = Q3_fare + 1.5 * IQR_fare\n",
    "\n",
    "print(f\"\\n--- FARE_AMOUNT ---\")\n",
    "print(f\"1er percentile: ${quantiles_fare_full[0]:.2f}\")\n",
    "print(f\"5e percentile: ${quantiles_fare_full[1]:.2f}\")\n",
    "print(f\"Q1 (25e percentile): ${quantiles_fare_full[2]:.2f}\")\n",
    "print(f\"M√©diane (50e percentile): ${quantiles_fare_full[3]:.2f}\")\n",
    "print(f\"Q3 (75e percentile): ${quantiles_fare_full[4]:.2f}\")\n",
    "print(f\"95e percentile: ${quantiles_fare_full[5]:.2f}\")\n",
    "print(f\"99e percentile: ${quantiles_fare_full[6]:.2f}\")\n",
    "print(f\"\\nIQR: ${IQR_fare:.2f}\")\n",
    "print(f\"Limites IQR (¬±1.5√óIQR): [${lower_bound_fare:.2f}, ${upper_bound_fare:.2f}]\")\n",
    "\n",
    "# Comptage outliers\n",
    "outliers_fare = df_population.filter(\n",
    "    (col(\"fare_amount\") < lower_bound_fare) | (col(\"fare_amount\") > upper_bound_fare)\n",
    ")\n",
    "nb_outliers_fare = outliers_fare.count()\n",
    "pct_outliers_fare = (nb_outliers_fare / nb_population) * 100\n",
    "\n",
    "print(f\"\\nüìä Outliers d√©tect√©s: {nb_outliers_fare:,} ({pct_outliers_fare:.2f}%)\")\n",
    "\n",
    "# Analyse des outliers extr√™mes (>99e percentile)\n",
    "print(\"\\n--- COURSES AVEC PRIX EXTR√äMES (>99e percentile) ---\")\n",
    "\n",
    "# V√©rifier si les colonnes borough existent\n",
    "if \"pickup_borough\" in df_population.columns:\n",
    "    extremes_high = df_population.filter(col(\"fare_amount\") > quantiles_fare_full[6]).select(\n",
    "        \"fare_amount\", \"trip_distance\", \"pickup_borough\", \"dropoff_borough\"\n",
    "    ).orderBy(col(\"fare_amount\").desc()).limit(10)\n",
    "else:\n",
    "    extremes_high = df_population.filter(col(\"fare_amount\") > quantiles_fare_full[6]).select(\n",
    "        \"fare_amount\", \"trip_distance\", \"PULocationID\", \"DOLocationID\"\n",
    "    ).orderBy(col(\"fare_amount\").desc()).limit(10)\n",
    "\n",
    "display(extremes_high)\n",
    "\n",
    "# Statistiques des outliers\n",
    "print(\"\\n--- STATISTIQUES DES OUTLIERS ---\")\n",
    "outliers_stats = outliers_fare.select(\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen_outliers\"),\n",
    "    mean(\"trip_distance\").alias(\"distance_moyenne_outliers\"),\n",
    "    spark_min(\"fare_amount\").alias(\"fare_min\"),\n",
    "    spark_max(\"fare_amount\").alias(\"fare_max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Prix moyen des outliers: ${outliers_stats['fare_moyen_outliers']:.2f}\")\n",
    "print(f\"Distance moyenne des outliers: {outliers_stats['distance_moyenne_outliers']:.2f} miles\")\n",
    "print(f\"Prix minimum: ${outliers_stats['fare_min']:.2f}\")\n",
    "print(f\"Prix maximum: ${outliers_stats['fare_max']:.2f}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.6 Ratio tip/fare par type de paiement (population)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "# Calcul exact pour toute la population\n",
    "df_population_ratio = df_population.filter(\n",
    "    (col(\"fare_amount\") > 0) & (col(\"tip_amount\") >= 0)\n",
    ").withColumn(\n",
    "    \"tip_ratio\", col(\"tip_amount\") / col(\"fare_amount\")\n",
    ")\n",
    "\n",
    "ratio_by_payment_exact = df_population_ratio.groupBy(\"payment_type\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"tip_ratio\").alias(\"ratio_tip_fare_moyen\"),\n",
    "    stddev(\"tip_ratio\").alias(\"std_ratio\"),\n",
    "    mean(\"tip_amount\").alias(\"tip_moyen\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\")\n",
    ").orderBy(col(\"nb_courses\").desc())\n",
    "\n",
    "print(\"=== RATIO TIP/FARE PAR TYPE DE PAIEMENT - POPULATION EXACTE ===\")\n",
    "print(\"Note: 1=Carte de cr√©dit, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip\")\n",
    "display(ratio_by_payment_exact)\n",
    "\n",
    "# Analyse d√©taill√©e\n",
    "print(\"\\n=== ANALYSE D√âTAILL√âE ===\")\n",
    "for row in ratio_by_payment_exact.collect():\n",
    "    payment_type = row['payment_type']\n",
    "    nb = row['nb_courses']\n",
    "    ratio = row['ratio_tip_fare_moyen']\n",
    "    tip_moyen = row['tip_moyen']\n",
    "    fare_moyen = row['fare_moyen']\n",
    "    \n",
    "    print(f\"\\nType de paiement {payment_type}:\")\n",
    "    print(f\"  ‚Ä¢ Nombre de courses: {nb:,}\")\n",
    "    print(f\"  ‚Ä¢ Ratio tip/fare: {ratio:.2%}\")\n",
    "    print(f\"  ‚Ä¢ Tip moyen: ${tip_moyen:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Fare moyen: ${fare_moyen:.2f}\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC_taxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
