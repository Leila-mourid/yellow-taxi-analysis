{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5e57f3-c75d-49ac-8b6f-d9dcbda3b384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # üöï Projet Taxi NYC - Analyse Comparative\n",
    "# MAGIC ## Statistiques Inf√©rentielles vs Big Data\n",
    "# MAGIC \n",
    "# MAGIC **Entreprise:** DATACO  \n",
    "# MAGIC **P√©riode:** 2022-2025  \n",
    "# MAGIC **Bin√¥me:** [Vos noms]  \n",
    "# MAGIC **Date:** 26-30 janvier 2026\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üì¶ 1. Configuration & Imports\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Imports PySpark\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, mean, stddev, sum as spark_sum, min as spark_min, max as spark_max,\n",
    "    hour, dayofweek, dayofmonth, month, year, weekofyear,\n",
    "    unix_timestamp, percentile_approx, when, lit\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Imports Python scientifique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "# Imports visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "# Configuration visualisation\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Configuration et imports termin√©s\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a7dd61-7249-4c0b-b9ce-ad19b336bdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üìÇ 2. Chargement des Donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, LongType\n",
    "\n",
    "# CHEMINS DES DONN√âES\n",
    "PATH_POPULATION = \"/Volumes/workspace/trips/population/\"\n",
    "PATH_SAMPLE = \"/Volumes/workspace/trips/sample/\"\n",
    "\n",
    "# ============================================\n",
    "# CHARGEMENT √âCHANTILLON (CSV)\n",
    "# ============================================\n",
    "print(\"üîÑ Chargement de l'√©chantillon...\")\n",
    "df_sample = spark.read.csv(\n",
    "    PATH_SAMPLE + \"yellowtaxisample1pct_hybrid_stratified.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "nb_sample = df_sample.count()\n",
    "print(f\"‚úÖ √âchantillon charg√© : {nb_sample:,} courses\")\n",
    "\n",
    "# ============================================\n",
    "# CHARGEMENT POPULATION - Approche fichier par fichier\n",
    "# ============================================\n",
    "print(\"\\nüîÑ Chargement de la population compl√®te...\")\n",
    "\n",
    "# Lister tous les fichiers parquet\n",
    "files = [f.path for f in dbutils.fs.ls(PATH_POPULATION) if f.path.endswith('.parquet')]\n",
    "print(f\"üìÇ {len(files)} fichiers trouv√©s\")\n",
    "\n",
    "# Charger tous les fichiers avec conversion automatique\n",
    "dfs = []\n",
    "for i, file_path in enumerate(files, 1):\n",
    "    try:\n",
    "        # Lire le fichier\n",
    "        df_temp = spark.read.parquet(file_path)\n",
    "        \n",
    "        # Convertir les colonnes qui peuvent √™tre INT64 ou DOUBLE\n",
    "        columns_to_convert = [\n",
    "            \"passenger_count\", \"trip_distance\", \"RatecodeID\",\n",
    "            \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \n",
    "            \"tolls_amount\", \"improvement_surcharge\", \"total_amount\",\n",
    "            \"congestion_surcharge\", \"airport_fee\"\n",
    "        ]\n",
    "        \n",
    "        for col_name in columns_to_convert:\n",
    "            if col_name in df_temp.columns:\n",
    "                df_temp = df_temp.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "        \n",
    "        dfs.append(df_temp)\n",
    "        print(f\"‚úÖ [{i}/{len(files)}] {file_path.split('/')[-1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur sur {file_path.split('/')[-1]}: {str(e)}\")\n",
    "\n",
    "# Fusionner tous les DataFrames\n",
    "if dfs:\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import DataFrame\n",
    "    \n",
    "    print(\"\\nüîÑ Fusion de tous les fichiers...\")\n",
    "    df_population = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "    \n",
    "    nb_population = df_population.count()\n",
    "    print(f\"‚úÖ Population totale charg√©e : {nb_population:,} courses\")\n",
    "else:\n",
    "    raise Exception(\"‚ùå Aucun fichier n'a pu √™tre charg√©\")\n",
    "\n",
    "# V√©rification ratio\n",
    "ratio = (nb_sample / nb_population) * 100\n",
    "print(f\"\\nüìä Ratio √©chantillon/population : {ratio:.2f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# V√©rification des sch√©mas finaux\n",
    "print(\"=== SCH√âMA POPULATION ===\")\n",
    "df_population.printSchema()\n",
    "\n",
    "print(\"\\n=== SCH√âMA √âCHANTILLON ===\")\n",
    "df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23e93f9-7ca9-4d50-8a06-2c656d1b921a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## üîç 3. EDA - Exploration des Donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.1 Structure des donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== SCH√âMA POPULATION ===\")\n",
    "df_population.printSchema()\n",
    "\n",
    "print(\"\\n=== SCH√âMA √âCHANTILLON ===\")\n",
    "df_sample.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.2 Aper√ßu des donn√©es\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== APER√áU POPULATION (5 premi√®res lignes) ===\")\n",
    "display(df_population.limit(5))\n",
    "\n",
    "print(\"\\n=== APER√áU √âCHANTILLON (5 premi√®res lignes) ===\")\n",
    "display(df_sample.limit(5))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.3 Statistiques descriptives de base\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Population\n",
    "print(\"=== STATISTIQUES DESCRIPTIVES - POPULATION ===\")\n",
    "stats_pop = df_population.select(\n",
    "    \"fare_amount\", \"trip_distance\", \"tip_amount\", \n",
    "    \"tolls_amount\", \"total_amount\", \"passenger_count\"\n",
    ").describe()\n",
    "display(stats_pop)\n",
    "\n",
    "# √âchantillon\n",
    "print(\"\\n=== STATISTIQUES DESCRIPTIVES - √âCHANTILLON ===\")\n",
    "stats_sample = df_sample.select(\n",
    "    \"fare_amount\", \"trip_distance\", \"tip_amount\", \n",
    "    \"tolls_amount\", \"total_amount\", \"passenger_count\"\n",
    ").describe()\n",
    "display(stats_sample)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.4 V√©rification des valeurs manquantes\n",
    "\n",
    "# COMMAND ----------\n",
    "from pyspark.sql.functions import col, sum as spark_sum, isnan, when, count\n",
    "# Fonction pour compter valeurs manquantes\n",
    "def count_nulls(df, dataset_name):\n",
    "    print(f\"\\n=== VALEURS MANQUANTES - {dataset_name} ===\")\n",
    "    null_counts = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "        for c in df.columns\n",
    "    ])\n",
    "    \n",
    "    # Conversion en pandas pour affichage plus lisible\n",
    "    null_df = null_counts.toPandas().T\n",
    "    null_df.columns = ['Nb_Nulls']\n",
    "    null_df['Pct_Nulls'] = (null_df['Nb_Nulls'] / df.count() * 100).round(2)\n",
    "    null_df = null_df[null_df['Nb_Nulls'] > 0].sort_values('Nb_Nulls', ascending=False)\n",
    "    \n",
    "    if len(null_df) > 0:\n",
    "        print(null_df)\n",
    "    else:\n",
    "        print(\"‚úÖ Aucune valeur manquante d√©tect√©e\")\n",
    "    \n",
    "    return null_df\n",
    "\n",
    "# V√©rification\n",
    "nulls_pop = count_nulls(df_population, \"POPULATION\")\n",
    "nulls_sample = count_nulls(df_sample, \"√âCHANTILLON\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3.5 D√©tection pr√©liminaire des outliers\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul des quartiles pour fare_amount\n",
    "quantiles_fare = df_population.approxQuantile(\"fare_amount\", [0.01, 0.25, 0.50, 0.75, 0.99], 0.01)\n",
    "Q1, Q3 = quantiles_fare[1], quantiles_fare[3]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"=== ANALYSE FARE_AMOUNT ===\")\n",
    "print(f\"Q1 (25%): ${Q1:.2f}\")\n",
    "print(f\"M√©diane (50%): ${quantiles_fare[2]:.2f}\")\n",
    "print(f\"Q3 (75%): ${Q3:.2f}\")\n",
    "print(f\"IQR: ${IQR:.2f}\")\n",
    "print(f\"1er percentile: ${quantiles_fare[0]:.2f}\")\n",
    "print(f\"99e percentile: ${quantiles_fare[4]:.2f}\")\n",
    "\n",
    "# Limites outliers (m√©thode IQR)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "print(f\"\\nLimites outliers (IQR ¬±1.5):\")\n",
    "print(f\"Limite inf√©rieure: ${lower_bound:.2f}\")\n",
    "print(f\"Limite sup√©rieure: ${upper_bound:.2f}\")\n",
    "\n",
    "# Comptage outliers\n",
    "nb_outliers = df_population.filter(\n",
    "    (col(\"fare_amount\") < lower_bound) | (col(\"fare_amount\") > upper_bound)\n",
    ").count()\n",
    "pct_outliers = (nb_outliers / nb_population) * 100\n",
    "\n",
    "print(f\"\\nüìä Outliers d√©tect√©s: {nb_outliers:,} ({pct_outliers:.2f}%)\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876d702a-8f3f-458c-8529-7ef1b9a4a4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## üìà 4. Analyse par Statistiques Inf√©rentielles (√âchantillon)\n",
    "\n",
    "# COMMAND ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, norm\n",
    "import scipy.stats as stats\n",
    "from pyspark.sql.functions import col, count, mean, stddev\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.1 Prix moyen avec intervalle de confiance\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Conversion en pandas pour calculs statistiques\n",
    "fare_sample_pd = df_sample.select(\"fare_amount\").toPandas()['fare_amount']\n",
    "\n",
    "# Statistiques de base\n",
    "n = len(fare_sample_pd)\n",
    "mean_fare = fare_sample_pd.mean()\n",
    "std_fare = fare_sample_pd.std()\n",
    "se_fare = std_fare / np.sqrt(n)\n",
    "\n",
    "# Intervalle de confiance 95% (distribution t de Student)\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "df_freedom = n - 1\n",
    "t_critical = t.ppf(1 - alpha/2, df_freedom)\n",
    "\n",
    "margin_error = t_critical * se_fare\n",
    "ic_inf = mean_fare - margin_error\n",
    "ic_sup = mean_fare + margin_error\n",
    "\n",
    "print(\"=== PRIX MOYEN (FARE_AMOUNT) - INF√âRENCE ===\")\n",
    "print(f\"Taille √©chantillon: {n:,}\")\n",
    "print(f\"Prix moyen estim√©: ${mean_fare:.2f}\")\n",
    "print(f\"√âcart-type: ${std_fare:.2f}\")\n",
    "print(f\"Erreur standard: ${se_fare:.4f}\")\n",
    "print(f\"t-critique (95%): {t_critical:.3f}\")\n",
    "print(f\"Marge d'erreur: ${margin_error:.2f}\")\n",
    "print(f\"\\nüéØ IC 95%: [${ic_inf:.2f}, ${ic_sup:.2f}]\")\n",
    "\n",
    "# Stockage pour comparaison\n",
    "results_inferential = {\n",
    "    'mean_fare': mean_fare,\n",
    "    'ic_fare_inf': ic_inf,\n",
    "    'ic_fare_sup': ic_sup\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.2 Distance moyenne avec IC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Distance moyenne\n",
    "distance_sample_pd = df_sample.select(\"trip_distance\").toPandas()['trip_distance']\n",
    "\n",
    "mean_distance = distance_sample_pd.mean()\n",
    "std_distance = distance_sample_pd.std()\n",
    "se_distance = std_distance / np.sqrt(n)\n",
    "margin_error_dist = t_critical * se_distance\n",
    "\n",
    "ic_dist_inf = mean_distance - margin_error_dist\n",
    "ic_dist_sup = mean_distance + margin_error_dist\n",
    "\n",
    "print(\"=== DISTANCE MOYENNE (TRIP_DISTANCE) - INF√âRENCE ===\")\n",
    "print(f\"Distance moyenne estim√©e: {mean_distance:.2f} miles\")\n",
    "print(f\"√âcart-type: {std_distance:.2f}\")\n",
    "print(f\"üéØ IC 95%: [{ic_dist_inf:.2f}, {ic_dist_sup:.2f}] miles\")\n",
    "\n",
    "results_inferential['mean_distance'] = mean_distance\n",
    "results_inferential['ic_dist_inf'] = ic_dist_inf\n",
    "results_inferential['ic_dist_sup'] = ic_dist_sup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.3 Dur√©e moyenne avec IC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul de la dur√©e en minutes\n",
    "df_sample_duration = df_sample.withColumn(\n",
    "    \"duration_minutes\",\n",
    "    (col(\"tpep_dropoff_datetime\").cast(\"long\")\n",
    "     - col(\"tpep_pickup_datetime\").cast(\"long\")) / 60\n",
    ")\n",
    "\n",
    "duration_sample_pd = df_sample_duration.select(\"duration_minutes\").toPandas()['duration_minutes']\n",
    "\n",
    "mean_duration = duration_sample_pd.mean()\n",
    "std_duration = duration_sample_pd.std()\n",
    "se_duration = std_duration / np.sqrt(n)\n",
    "margin_error_dur = t_critical * se_duration\n",
    "\n",
    "ic_dur_inf = mean_duration - margin_error_dur\n",
    "ic_dur_sup = mean_duration + margin_error_dur\n",
    "\n",
    "print(\"=== DUR√âE MOYENNE - INF√âRENCE ===\")\n",
    "print(f\"Dur√©e moyenne estim√©e: {mean_duration:.2f} minutes\")\n",
    "print(f\"√âcart-type: {std_duration:.2f}\")\n",
    "print(f\"üéØ IC 95%: [{ic_dur_inf:.2f}, {ic_dur_sup:.2f}] minutes\")\n",
    "\n",
    "results_inferential['mean_duration'] = mean_duration\n",
    "results_inferential['ic_dur_inf'] = ic_dur_inf\n",
    "results_inferential['ic_dur_sup'] = ic_dur_sup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.4 Proportion avec tip > 0 et IC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Proportion dans l'√©chantillon\n",
    "nb_with_tip = df_sample.filter(col(\"tip_amount\") > 0).count()\n",
    "p_sample = nb_with_tip / n\n",
    "\n",
    "# IC pour proportion (approximation normale)\n",
    "z_critical = norm.ppf(1 - alpha/2)  # 1.96 pour 95%\n",
    "se_prop = np.sqrt(p_sample * (1 - p_sample) / n)\n",
    "margin_error_prop = z_critical * se_prop\n",
    "\n",
    "ic_prop_inf = max(0, p_sample - margin_error_prop)\n",
    "ic_prop_sup = min(1, p_sample + margin_error_prop)\n",
    "\n",
    "print(\"=== PROPORTION AVEC TIP > 0 - INF√âRENCE ===\")\n",
    "print(f\"Nombre avec tip: {nb_with_tip:,}\")\n",
    "print(f\"Proportion estim√©e: {p_sample:.2%}\")\n",
    "print(f\"z-critique (95%): {z_critical:.3f}\")\n",
    "print(f\"üéØ IC 95%: [{ic_prop_inf:.2%}, {ic_prop_sup:.2%}]\")\n",
    "\n",
    "results_inferential['prop_tip'] = p_sample\n",
    "results_inferential['ic_prop_inf'] = ic_prop_inf\n",
    "results_inferential['ic_prop_sup'] = ic_prop_sup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.5 Ratio tip/fare moyen par type de paiement\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul du ratio tip/fare pour √©chantillon\n",
    "df_sample_ratio = df_sample.filter(\n",
    "    (col(\"fare_amount\") > 0) & (col(\"tip_amount\") >= 0)\n",
    ").withColumn(\n",
    "    \"tip_ratio\", col(\"tip_amount\") / col(\"fare_amount\")\n",
    ")\n",
    "\n",
    "# Groupement par type de paiement\n",
    "ratio_by_payment = df_sample_ratio.groupBy(\"payment_type\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"tip_ratio\").alias(\"ratio_tip_fare_moyen\"),\n",
    "    stddev(\"tip_ratio\").alias(\"std_ratio\")\n",
    ").orderBy(col(\"nb_courses\").desc())\n",
    "\n",
    "print(\"=== RATIO TIP/FARE PAR TYPE DE PAIEMENT - INF√âRENCE ===\")\n",
    "display(ratio_by_payment)\n",
    "\n",
    "# Pour chaque type de paiement, calculer IC\n",
    "ratio_pd = df_sample_ratio.select(\"payment_type\", \"tip_ratio\").toPandas()\n",
    "for payment_type in ratio_pd['payment_type'].unique():\n",
    "    data = ratio_pd[ratio_pd['payment_type'] == payment_type]['tip_ratio']\n",
    "    if len(data) > 30:  # Nombre suffisant\n",
    "        mean_r = data.mean()\n",
    "        std_r = data.std()\n",
    "        n_r = len(data)\n",
    "        se_r = std_r / np.sqrt(n_r)\n",
    "        me_r = t.ppf(0.975, n_r-1) * se_r\n",
    "        print(f\"Payment {payment_type}: {mean_r:.2%} ¬± {me_r:.2%}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4.6 Test d'hypoth√®se : Comparaison Manhattan vs Brooklyn\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Filtrage des prix valides\n",
    "card_fares = df_sample.filter(\n",
    "    (col(\"payment_type\") == 1) & (col(\"fare_amount\") > 0)\n",
    ").select(\"fare_amount\").toPandas()['fare_amount']\n",
    "\n",
    "cash_fares = df_sample.filter(\n",
    "    (col(\"payment_type\") == 2) & (col(\"fare_amount\") > 0)\n",
    ").select(\"fare_amount\").toPandas()['fare_amount']\n",
    "\n",
    "# Test t de Student (√©chantillons ind√©pendants)\n",
    "if len(card_fares) > 30 and len(cash_fares) > 30:\n",
    "    t_stat, p_value = stats.ttest_ind(card_fares, cash_fares, equal_var=False)\n",
    "\n",
    "    print(\"=== TEST D'HYPOTH√àSE : CARTE vs CASH ===\")\n",
    "    print(f\"Carte - Prix moyen: ${card_fares.mean():.2f} (n={len(card_fares)})\")\n",
    "    print(f\"Cash  - Prix moyen: ${cash_fares.mean():.2f} (n={len(cash_fares)})\")\n",
    "    print(f\"\\nt-statistique: {t_stat:.3f}\")\n",
    "    print(f\"p-value: {p_value:.6f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n‚úÖ Diff√©rence SIGNIFICATIVE (p < 0.05)\")\n",
    "        print(\"üëâ Le mode de paiement influence le prix moyen de la course\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Diff√©rence NON significative (p ‚â• 0.05)\")\n",
    "        print(\"üëâ Aucune preuve d‚Äôun effet du mode de paiement\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Taille d‚Äô√©chantillon insuffisante pour le test\")\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7ed46a-6b64-43d8-b0a4-4d08579e5cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## üíæ 5. Analyse Big Data (Population Compl√®te)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.1 Calcul des m√©triques EXACTES\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "\n",
    "# Mesure du temps de calcul\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=== CALCUL DES M√âTRIQUES EXACTES (POPULATION) ===\")\n",
    "print(\"üîÑ Calcul en cours...\")\n",
    "\n",
    "# Prix moyen exact\n",
    "mean_fare_exact = df_population.agg(mean(\"fare_amount\")).collect()[0][0]\n",
    "print(f\"‚úÖ Prix moyen EXACT: ${mean_fare_exact:.2f}\")\n",
    "\n",
    "# Distance moyenne exacte\n",
    "mean_distance_exact = df_population.agg(mean(\"trip_distance\")).collect()[0][0]\n",
    "print(f\"‚úÖ Distance moyenne EXACTE: {mean_distance_exact:.2f} miles\")\n",
    "\n",
    "# Dur√©e moyenne exacte\n",
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# 1Ô∏è‚É£ Cr√©ation de la colonne dur√©e en minutes\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_population_duration = df_population_duration.withColumn(\n",
    "    \"duration_minutes\",\n",
    "    (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / 60\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ V√©rification rapide\n",
    "df_population_duration.select(\"duration_minutes\").show(5)\n",
    "\n",
    "# 3Ô∏è‚É£ Calcul de la moyenne exacte\n",
    "mean_duration_exact = df_population_duration.agg(\n",
    "    mean(\"duration_minutes\")\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"Dur√©e moyenne exacte (population) : {mean_duration_exact:.2f} minutes\")\n",
    "\n",
    "\n",
    "# Proportion exacte avec tip\n",
    "nb_with_tip_exact = df_population.filter(col(\"tip_amount\") > 0).count()\n",
    "prop_tip_exact = nb_with_tip_exact / nb_population\n",
    "print(f\"‚úÖ Proportion EXACTE avec tip: {prop_tip_exact:.2%}\")\n",
    "\n",
    "# Temps de calcul\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è Temps de calcul: {elapsed_time:.2f} secondes\")\n",
    "\n",
    "# Stockage pour comparaison\n",
    "results_bigdata = {\n",
    "    'mean_fare': mean_fare_exact,\n",
    "    'mean_distance': mean_distance_exact,\n",
    "    'mean_duration': mean_duration_exact,\n",
    "    'prop_tip': prop_tip_exact,\n",
    "    'compute_time': elapsed_time\n",
    "}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.2 Distribution temporelle - Heures de pointe\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Ajout des colonnes temporelles\n",
    "df_population_time = df_population.withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n",
    "                                   .withColumn(\"dayofweek\", dayofweek(\"pickup_datetime\")) \\\n",
    "                                   .withColumn(\"day\", dayofmonth(\"pickup_datetime\")) \\\n",
    "                                   .withColumn(\"month\", month(\"pickup_datetime\"))\n",
    "\n",
    "# Analyse par heure\n",
    "courses_par_heure = df_population_time.groupBy(\"hour\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "    mean(\"trip_distance\").alias(\"distance_moyenne\")\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "print(\"=== DISTRIBUTION PAR HEURE ===\")\n",
    "display(courses_par_heure)\n",
    "\n",
    "# Identification heures de pointe\n",
    "print(\"\\n=== TOP 5 HEURES DE POINTE ===\")\n",
    "heures_pointe = courses_par_heure.orderBy(col(\"nb_courses\").desc()).limit(5)\n",
    "display(heures_pointe)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.3 Distribution par jour de la semaine\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Analyse par jour de semaine (1=Dimanche, 7=Samedi)\n",
    "courses_par_jour = df_population_time.groupBy(\"dayofweek\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\")\n",
    ").orderBy(\"dayofweek\")\n",
    "\n",
    "# Ajout noms jours\n",
    "jours_mapping = {\n",
    "    1: \"Dimanche\", 2: \"Lundi\", 3: \"Mardi\", 4: \"Mercredi\",\n",
    "    5: \"Jeudi\", 6: \"Vendredi\", 7: \"Samedi\"\n",
    "}\n",
    "\n",
    "print(\"=== DISTRIBUTION PAR JOUR DE LA SEMAINE ===\")\n",
    "display(courses_par_jour)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.4 Analyse g√©ographique par boroughs\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Analyse compl√®te par borough\n",
    "analyse_boroughs = df_population.groupBy(\"pickup_borough\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "    mean(\"trip_distance\").alias(\"distance_moyenne\"),\n",
    "    mean(\"tip_amount\").alias(\"tip_moyen\"),\n",
    "    (mean(\"tip_amount\") / mean(\"fare_amount\") * 100).alias(\"tip_pct_fare\")\n",
    ").orderBy(col(\"nb_courses\").desc())\n",
    "\n",
    "print(\"=== ANALYSE PAR BOROUGH (PICKUP) ===\")\n",
    "display(analyse_boroughs)\n",
    "\n",
    "# Statistiques par paire origine-destination\n",
    "top_routes = df_population.groupBy(\"pickup_borough\", \"dropoff_borough\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\"),\n",
    "    mean(\"trip_distance\").alias(\"distance_moyenne\")\n",
    ").orderBy(col(\"nb_courses\").desc()).limit(10)\n",
    "\n",
    "print(\"\\n=== TOP 10 ROUTES (ORIGINE ‚Üí DESTINATION) ===\")\n",
    "display(top_routes)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.5 D√©tection et analyse des outliers\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul des percentiles pour plusieurs variables\n",
    "print(\"=== ANALYSE DES OUTLIERS ===\")\n",
    "\n",
    "# Fare amount\n",
    "quantiles_fare_full = df_population.approxQuantile(\"fare_amount\", [0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99], 0.01)\n",
    "Q1_fare, Q3_fare = quantiles_fare_full[2], quantiles_fare_full[4]\n",
    "IQR_fare = Q3_fare - Q1_fare\n",
    "lower_bound_fare = Q1_fare - 1.5 * IQR_fare\n",
    "upper_bound_fare = Q3_fare + 1.5 * IQR_fare\n",
    "\n",
    "print(f\"\\n--- FARE_AMOUNT ---\")\n",
    "print(f\"1er percentile: ${quantiles_fare_full[0]:.2f}\")\n",
    "print(f\"5e percentile: ${quantiles_fare_full[1]:.2f}\")\n",
    "print(f\"95e percentile: ${quantiles_fare_full[5]:.2f}\")\n",
    "print(f\"99e percentile: ${quantiles_fare_full[6]:.2f}\")\n",
    "print(f\"Limites IQR: [${lower_bound_fare:.2f}, ${upper_bound_fare:.2f}]\")\n",
    "\n",
    "# Comptage outliers\n",
    "outliers_fare = df_population.filter(\n",
    "    (col(\"fare_amount\") < lower_bound_fare) | (col(\"fare_amount\") > upper_bound_fare)\n",
    ")\n",
    "nb_outliers_fare = outliers_fare.count()\n",
    "pct_outliers_fare = (nb_outliers_fare / nb_population) * 100\n",
    "\n",
    "print(f\"Outliers: {nb_outliers_fare:,} ({pct_outliers_fare:.2f}%)\")\n",
    "\n",
    "# Analyse des outliers extr√™mes\n",
    "print(\"\\n--- COURSES AVEC PRIX EXTR√äMES ---\")\n",
    "extremes_high = df_population.filter(col(\"fare_amount\") > quantiles_fare_full[6]).select(\n",
    "    \"fare_amount\", \"trip_distance\", \"pickup_borough\", \"dropoff_borough\"\n",
    ").limit(10)\n",
    "display(extremes_high)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5.6 Ratio tip/fare par type de paiement (population)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calcul exact pour toute la population\n",
    "df_population_ratio = df_population.filter(\n",
    "    (col(\"fare_amount\") > 0) & (col(\"tip_amount\") >= 0)\n",
    ").withColumn(\n",
    "    \"tip_ratio\", col(\"tip_amount\") / col(\"fare_amount\")\n",
    ")\n",
    "\n",
    "ratio_by_payment_exact = df_population_ratio.groupBy(\"payment_type\").agg(\n",
    "    count(\"*\").alias(\"nb_courses\"),\n",
    "    mean(\"tip_ratio\").alias(\"ratio_tip_fare_moyen\"),\n",
    "    stddev(\"tip_ratio\").alias(\"std_ratio\"),\n",
    "    mean(\"tip_amount\").alias(\"tip_moyen\"),\n",
    "    mean(\"fare_amount\").alias(\"fare_moyen\")\n",
    ").orderBy(col(\"nb_courses\").desc())\n",
    "\n",
    "print(\"=== RATIO TIP/FARE PAR TYPE DE PAIEMENT - POPULATION EXACTE ===\")\n",
    "display(ratio_by_payment_exact)\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NYC_taxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
